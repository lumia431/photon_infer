# PhotonInfer

<div align="center">

**A High-Performance LLM Inference Engine with vLLM-Style Continuous Batching**

[English](README.md) | [ä¸­æ–‡](README_ZH.md)

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![CUDA](https://img.shields.io/badge/CUDA-12.0+-green.svg)](https://developer.nvidia.com/cuda-toolkit)
[![C++20](https://img.shields.io/badge/C++-20-orange.svg)](https://en.cppreference.com/w/cpp/20)

</div>

---

## ğŸš€ Performance Highlights

PhotonInfer delivers **production-grade inference performance** for LLMs with advanced batching capabilities:

| Metric | Performance | Comparison |
|--------|------------|------------|
| **Peak Throughput** | **518 tokens/s** @ batch=16 | **2.05Ã—** faster than llama.cpp |
| **Batch Scaling** | 71 â†’ 518 tokens/s (7.3Ã—) | Linear scaling up to batch=16 |
| **Continuous Batching** | **2.02Ã— throughput**, 2.59Ã— lower latency | Unique advantage over baseline |
| **Urgent Request Latency** | **0.29s** vs 755s (baseline) | **2500Ã—** improvement |

**Tested on**: NVIDIA RTX 5060 Ti, Llama 3.2 1B, Q8/INT8 quantization

## âœ¨ Key Features

### ğŸ¯ **vLLM-Style Continuous Batching**
- **Token-level dynamic scheduling**: Add new requests mid-generation without waiting for batch completion
- **Two-phase scheduler**: Seamlessly continue running requests while admitting new ones
- **Request state tracking**: Precise `num_computed_tokens` management for efficient resume
- **Perfect for production**: High-concurrency inference services with real-time responsiveness

### âš¡ **GPU-Optimized Kernels**
- **Batched Paged Attention**: Block-level KV cache management with efficient memory utilization
- **Vectorized Memory Access**: `float4` loads for 2-4Ã— bandwidth efficiency
- **Fused Operations**: Zero-copy GPU sampling, batched RoPE, and fused normalization
- **INT8 Quantization**: Group-wise quantization with cuBLASLt INT8 GEMM support
- **Optimized Softmax**: CUB BlockReduce for numerically stable attention computation

### ğŸ—ï¸ **Modern C++20 Architecture**
- **Type-Safe Error Handling**: Rust-inspired `Result<T, E>` type for explicit error propagation
- **Zero-Copy Design**: Extensive use of `std::span` and move semantics
- **Device Agnostic**: Unified interface for CPU and CUDA backends
- **Concepts & Ranges**: Compile-time constraints and expressive type safety

## ğŸ“Š Benchmark Results

### Batch Inference Throughput

```
Tokens/s
600 â”¤                                                â•­â”€â”€â”€ PhotonInfer
    â”‚                                           â•­â”€â”€â”€â”€â•¯
500 â”¤                                      â•­â”€â”€â”€â”€â•¯
    â”‚                                 â•­â”€â”€â”€â”€â•¯
400 â”¤                            â•­â”€â”€â”€â”€â•¯
    â”‚                       â•­â”€â”€â”€â”€â•¯
300 â”¤                  â•­â”€â”€â”€â”€â•¯
    â”‚  llama.cpp â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
200 â”¤
    â”‚
100 â”¤
    â”‚
  0 â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€
    1        2        4        8        16
                   Batch Size
```

**PhotonInfer dominates at batch â‰¥ 4 with true parallel batch processing**

### Continuous Batching Advantage

| Scenario | Baseline (Wait) | Continuous Batching | Improvement |
|----------|----------------|---------------------|-------------|
| **Throughput** | 236 tokens/s | 477 tokens/s | **2.02Ã—** |
| **Average Latency** | 3.27s | 1.26s | **2.59Ã—** |
| **Urgent Request** | 755s | 0.29s | **2500Ã—+** |

## ğŸ¯ Use Cases

**PhotonInfer excels at:**
- âœ… High-concurrency inference services (4+ concurrent requests)
- âœ… Real-time interactive applications requiring low latency
- âœ… Production deployments prioritizing overall throughput
- âœ… Dynamic workloads with varying request arrival patterns

**Choose llama.cpp for:**
- ğŸ“± Single-user local applications
- ğŸ’» Low-concurrency scenarios (1-3 requests)
- ğŸ”‹ Resource-constrained environments

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Continuous Batch Engine                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         Two-Phase Scheduler                         â”‚   â”‚
â”‚  â”‚  â€¢ RUNNING requests (continue generation)           â”‚   â”‚
â”‚  â”‚  â€¢ WAITING requests (fill remaining capacity)       â”‚   â”‚
â”‚  â”‚  â€¢ Token-level preemption support                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         Transformer Layers (Batched)                â”‚   â”‚
â”‚  â”‚  â€¢ Batched RMSNorm (fused)                          â”‚   â”‚
â”‚  â”‚  â€¢ INT8 Quantized MatMul (cuBLASLt)                 â”‚   â”‚
â”‚  â”‚  â€¢ Batched RoPE (fused)                             â”‚   â”‚
â”‚  â”‚  â€¢ Paged Multi-Head Attention                       â”‚   â”‚
â”‚  â”‚    - Vectorized K/V cache access (float4)           â”‚   â”‚
â”‚  â”‚    - Optimized softmax (CUB reduce)                 â”‚   â”‚
â”‚  â”‚    - Partitioned attention for long sequences       â”‚   â”‚
â”‚  â”‚  â€¢ SwiGLU FFN                                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         GPU Sampling (Zero-Copy)                    â”‚   â”‚
â”‚  â”‚  â€¢ Batched temperature scaling                      â”‚   â”‚
â”‚  â”‚  â€¢ Top-p/top-k filtering                            â”‚   â”‚
â”‚  â”‚  â€¢ Categorical sampling on GPU                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- **Compiler**: GCC 12+ (C++20 support required)
- **CMake**: 3.20+
- **CUDA Toolkit**: 11.0+ (tested on 12.0)
- **GPU**: NVIDIA GPU with Compute Capability 7.0+

### Build

```bash
# Clone repository
cd photon_infer

# Configure with CUDA
mkdir build && cd build
cmake -DCMAKE_BUILD_TYPE=Release -DPHOTON_BUILD_CUDA=ON ..

# Build
cmake --build . -j$(nproc)
```

### Run Inference

```bash
# Single request inference
./bin/llama_infer

# Batched inference (4 concurrent requests)
./bin/batched_inference_demo

# Continuous batching demo (compare with baseline)
./bin/compare_batching_methods

# Comprehensive benchmark
./bin/benchmark_photon
```

### Example: Continuous Batching Engine

```cpp
#include "photon/scheduler/continuous_batch_engine.hpp"

using namespace photon::scheduler;

// Initialize engine
ContinuousBatchEngine engine(model, max_batch_size, max_seq_len);

// Add requests dynamically (non-blocking)
auto req1 = engine.add_request(prompt_tokens_1, max_new_tokens);
auto req2 = engine.add_request(prompt_tokens_2, max_new_tokens);

// Engine automatically schedules and executes
engine.step();  // Process one token for entire batch

// Retrieve results as they complete
if (req1->is_finished()) {
  auto tokens = req1->generated_tokens();
  std::string text = tokenizer.decode(tokens);
}
```

## ğŸ“ Project Structure

```
photon_infer/
â”œâ”€â”€ include/photon/
â”‚   â”œâ”€â”€ core/                    # Core abstractions
â”‚   â”‚   â”œâ”€â”€ types.hpp           # Type system with C++20 concepts
â”‚   â”‚   â”œâ”€â”€ error.hpp           # Result<T> error handling
â”‚   â”‚   â”œâ”€â”€ tensor.hpp          # N-dimensional tensor
â”‚   â”‚   â””â”€â”€ allocator.hpp       # Device memory allocators
â”‚   â”œâ”€â”€ ops/                     # Operators
â”‚   â”‚   â”œâ”€â”€ matmul.hpp          # INT8 quantized matrix multiplication
â”‚   â”‚   â”œâ”€â”€ mha.hpp             # Multi-head attention
â”‚   â”‚   â”œâ”€â”€ rope.hpp            # Rotary position embedding
â”‚   â”‚   â””â”€â”€ kernels/cuda/       # CUDA kernel implementations
â”‚   â”œâ”€â”€ arch/                    # Model architecture
â”‚   â”‚   â”œâ”€â”€ llama_model.hpp     # LLaMA transformer model
â”‚   â”‚   â”œâ”€â”€ transformer_block.hpp
â”‚   â”‚   â””â”€â”€ config.hpp          # Model configuration
â”‚   â”œâ”€â”€ runtime/                 # Runtime components
â”‚   â”‚   â””â”€â”€ kv_cache_manager.hpp # Paged KV cache
â”‚   â”œâ”€â”€ io/                      # Input/Output
â”‚   â”‚   â”œâ”€â”€ checkpoint.hpp      # Checkpoint loader
â”‚   â”‚   â”œâ”€â”€ model_loader.hpp    # mmap-based model loading
â”‚   â”‚   â””â”€â”€ tokenizer.hpp       # TikToken tokenizer
â”‚   â””â”€â”€ scheduler/               # Continuous batching scheduler
â”‚       â”œâ”€â”€ inference_request.hpp
â”‚       â”œâ”€â”€ continuous_batch_scheduler.hpp
â”‚       â””â”€â”€ continuous_batch_engine.hpp
â”œâ”€â”€ src/                         # Implementation files
â”œâ”€â”€ demo/                        # Demo applications
â”‚   â”œâ”€â”€ compare_batching_methods.cpp  # Baseline vs continuous batching
â”‚   â”œâ”€â”€ benchmark_photon.cpp          # Comprehensive benchmarks
â”‚   â””â”€â”€ batched_inference_demo.cpp    # Multi-request inference
â””â”€â”€ tests/                       # Unit tests (Google Test)
```

## ğŸ”¬ Technical Details

### INT8 Quantization
- **Group-wise quantization**: Configurable group size (32, 64, 128)
- **cuBLASLt integration**: Hardware-accelerated INT8 GEMM
- **Minimal accuracy loss**: < 1% perplexity degradation on Llama models

### Paged Attention
- **Block-level KV cache**: Efficient memory allocation without fragmentation
- **Dynamic sequence management**: Per-sequence cache offsets for flexible scheduling
- **Batched cache operations**: Single kernel for multi-sequence K/V writes

### Continuous Batching Scheduler
- **Two-phase scheduling**:
  1. **Phase 1**: Continue all RUNNING requests (no interruption)
  2. **Phase 2**: Admit WAITING requests to fill remaining capacity
- **Request states**: WAITING â†’ RUNNING â†’ FINISHED (with PREEMPTED support)
- **Token-level granularity**: `num_computed_tokens` tracking for precise resume

## ğŸ“Š Performance Comparison

### vs llama.cpp (Q8_0, Llama 3.2 1B, RTX 5060 Ti)

| Batch Size | PhotonInfer | llama.cpp | Speedup |
|------------|-------------|-----------|---------|
| 1          | 71 tok/s    | 252 tok/s | 0.28Ã— (llama.cpp faster) |
| 2          | 134 tok/s   | 252 tok/s | 0.53Ã— |
| 4          | 273 tok/s   | 252 tok/s | **1.08Ã—** |
| 8          | 480 tok/s   | 255 tok/s | **1.88Ã—** |
| 16         | 518 tok/s   | 253 tok/s | **2.05Ã—** |

**Key observation**: llama.cpp's decode performance is **constant across batch sizes** (~252 tok/s), indicating serial processing. PhotonInfer achieves **true parallel batching** with linear scaling.

## ğŸ›£ï¸ Roadmap

- [x] **Core Infrastructure**: Tensor, operators, memory management
- [x] **LLaMA Model**: Full transformer implementation with CPU/GPU kernels
- [x] **INT8 Quantization**: Group-wise quantization with cuBLASLt
- [x] **Paged Attention**: Block-level KV cache management
- [x] **Continuous Batching**: vLLM-style dynamic request scheduling
- [ ] **Flash Attention 2**: IO-aware attention for long sequences
- [ ] **Multi-GPU Support**: Tensor parallelism for large models
- [ ] **FP16/BF16 Mixed Precision**: Enhanced throughput on modern GPUs
- [ ] **Speculative Decoding**: Multi-token generation with draft model

## ğŸ“– Documentation

- [Continuous Batching Design](docs/continuous_batching.md)
- [Performance Optimization Guide](docs/performance.md)
- [API Reference](docs/api.md)

## ğŸ¤ Contributing

Contributions welcome! Please see [CONTRIBUTING.md](docs/contributing.md) for guidelines.

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) for details.

## ğŸ™ Acknowledgments

- Architecture inspired by [vLLM](https://github.com/vllm-project/vllm)
- Kernel optimizations reference [llama.cpp](https://github.com/ggerganov/llama.cpp)
- Error handling design from Rust's `Result<T, E>`

---

**Built with â¤ï¸ for high-performance LLM inference**
